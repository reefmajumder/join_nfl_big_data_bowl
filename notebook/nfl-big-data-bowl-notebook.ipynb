{"metadata":{"autobot":{"authors":[{"author":"Alec Kerrigan","github":"ahkerrigan","web":null}],"categories":["fa19"],"date":"2019-10-17","description":"AI@UCF has started its first data science competition! The National Football League's  $75,000 competition challenges you with using player stats, position, and time data to predict the future number of yards gained. Come join us as we dive into this competition and learn how to get started.","tags":["machine-learning","data science","football","competition"],"title":"Introducing the NFL Big Data Bowl"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://ucfai.org/data-science/fa19/2019-10-17-nfl-intro/nfl-intro/banner.png\">\n\n\n\n<div class=\"col-12\">\n    <h1> Introducing the NFL Big Data Bowl </h1>\n    <hr>\n</div>\n\n<div style=\"line-height: 2em;\">\n    <p>by: \n        <strong> Reefayat Majumder </strong>\n        (<a href=\"https://github.com/reefmajumder/join_nfl_big_data_bowl\">@reefmajumder</a>)\n     on 2021-11-17</p>\n</div>","metadata":{"nb-title":true,"title":"Introducing the NFL Big Data Bowl"}},{"cell_type":"markdown","source":"### Importing Required Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom kaggle.competitions import nflrush\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom keras import Sequential\nfrom keras.layers import Dense,BatchNormalization,Dropout\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.callbacks import ReduceLROnPlateau\nimport datetime\nimport tqdm","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2021-11-24T03:34:51.678088Z","iopub.execute_input":"2021-11-24T03:34:51.678434Z","iopub.status.idle":"2021-11-24T03:34:54.955309Z","shell.execute_reply.started":"2021-11-24T03:34:51.678380Z","shell.execute_reply":"2021-11-24T03:34:54.954125Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Getting a basic Idea\n- Each row of the train dataframe contains the attributes of a single player in the match with the target value.\n- There are 22 players in a single game.\n- Each of this 22 player participating in game has a row.\n- This mean that we have 509762 / 22 = 23171 samples effectively.\n- There are many missing values in different columns,WindSpeed, WindDirection, Temperature, GameWeather, Humidity, StadiumType, and FieldPosition.\n- We will preprocess the data and make it in a trainable form.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Setting up the environment\n#### Its important to note that this competition has its own API.\nThis time around, we aren't going to concatonate train and test, because the way data is handled\nhere is a little wonky. This means after we feature engineer and clean the train, we, yes, have\nto do it all over again to the test.","metadata":{}},{"cell_type":"code","source":"# First, let's build the enviroment from the API\nenv=nflrush.make_env()\ndf_train=pd.read_csv('../input/nfl-big-data-bowl-2020/train.csv',low_memory=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:34:54.957449Z","iopub.execute_input":"2021-11-24T03:34:54.957800Z","iopub.status.idle":"2021-11-24T03:35:03.438311Z","shell.execute_reply.started":"2021-11-24T03:34:54.957739Z","shell.execute_reply":"2021-11-24T03:35:03.437258Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:35:03.442277Z","iopub.execute_input":"2021-11-24T03:35:03.442544Z","iopub.status.idle":"2021-11-24T03:35:03.746077Z","shell.execute_reply.started":"2021-11-24T03:35:03.442494Z","shell.execute_reply":"2021-11-24T03:35:03.745436Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print('The train dataframe contrains {} rows and {} columns'.format(df_train.shape[0],df_train.shape[1]))\n","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:35:03.748424Z","iopub.execute_input":"2021-11-24T03:35:03.748884Z","iopub.status.idle":"2021-11-24T03:35:03.753293Z","shell.execute_reply.started":"2021-11-24T03:35:03.748839Z","shell.execute_reply":"2021-11-24T03:35:03.752584Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"##  handling missing values...\n\n","metadata":{}},{"cell_type":"code","source":"df_train.isna().sum().sort_values(ascending=False)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-24T03:35:03.756483Z","iopub.execute_input":"2021-11-24T03:35:03.756834Z","iopub.status.idle":"2021-11-24T03:35:04.563343Z","shell.execute_reply.started":"2021-11-24T03:35:03.756778Z","shell.execute_reply":"2021-11-24T03:35:04.562314Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Let's start by trying to fix windspeed","metadata":{}},{"cell_type":"code","source":"df_train['WindSpeed'].value_counts()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-24T03:35:04.565314Z","iopub.execute_input":"2021-11-24T03:35:04.565894Z","iopub.status.idle":"2021-11-24T03:35:04.633358Z","shell.execute_reply.started":"2021-11-24T03:35:04.565662Z","shell.execute_reply":"2021-11-24T03:35:04.632406Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Alright so this data is not very consistent with how wind speed is labeled, so lets fix that","metadata":{}},{"cell_type":"markdown","source":"- There are many different type of values in Windspeed,we will clean them.\n- We will remove mph/MPH if its present\n- we will return average value if there is a range of values eg(11 - 15).\n","metadata":{}},{"cell_type":"code","source":"def windspeed(x):\n    x=str(x)\n    if x.isdigit():\n        return int(x)\n    elif (x.isalpha()):\n        return 0\n    elif (x.isalnum()):\n        return int(x.upper().split('M')[0])                             #return 12 incase of 12mp or 12 MPH\n    elif '-' in x:\n        return int((int(x.split('-')[0])+int(x.split('-')[1]))/2)   # return average windspeed incase of 11 - 20 etc..\n    else:\n        return 0","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:35:04.635331Z","iopub.execute_input":"2021-11-24T03:35:04.635744Z","iopub.status.idle":"2021-11-24T03:35:04.646000Z","shell.execute_reply.started":"2021-11-24T03:35:04.635649Z","shell.execute_reply":"2021-11-24T03:35:04.644810Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### We are going to use a very nice pandas feature that YOU ALL should know\nBy using .apply(function) to some column in a dataframe, you can apply that function individually to all items ","metadata":{}},{"cell_type":"code","source":"# Let's just apply our fix to the messed up values \ndf_train['WindSpeed']=df_train['WindSpeed'].apply(windspeed)\n# Then, lets just fill the missing values with the average, as we have been doing \ndf_train['WindSpeed'].fillna(df_train['WindSpeed'].mean(),inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:35:04.647610Z","iopub.execute_input":"2021-11-24T03:35:04.648251Z","iopub.status.idle":"2021-11-24T03:35:05.408460Z","shell.execute_reply.started":"2021-11-24T03:35:04.648189Z","shell.execute_reply":"2021-11-24T03:35:05.407330Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"sns.distplot(df_train['WindSpeed'])","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:35:05.409993Z","iopub.execute_input":"2021-11-24T03:35:05.410434Z","iopub.status.idle":"2021-11-24T03:35:05.833643Z","shell.execute_reply.started":"2021-11-24T03:35:05.410322Z","shell.execute_reply":"2021-11-24T03:35:05.832775Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Yeh,We have cleaned all the windspeed values.\n\n### WindDirection values\n\nWind direction was much worse, so let's take a look at that","metadata":{}},{"cell_type":"code","source":"df_train['WindDirection'].value_counts()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-24T03:35:05.835660Z","iopub.execute_input":"2021-11-24T03:35:05.835973Z","iopub.status.idle":"2021-11-24T03:35:05.901089Z","shell.execute_reply.started":"2021-11-24T03:35:05.835917Z","shell.execute_reply":"2021-11-24T03:35:05.900040Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# So wind direction is a bit confusing in that it is all base on WHERE it comes from\n# We can see that it has the same problem as wind speed: multiple wants for saying the same thing.\n# So we need to handle these cases indv\n# We are going to reduce the number of options a bit \n\ndef clean_wind_direction(wind_direction):\n    wd = str(wind_direction).upper()\n    if wd == 'N' or 'FROM N' in wd:\n        return 'north'\n    if wd == 'S' or 'FROM S' in wd:\n        return 'south'\n    if wd == 'W' or 'FROM W' in wd:\n        return 'west'\n    if wd == 'E' or 'FROM E' in wd:\n        return 'east'\n    \n    if 'FROM SW' in wd or 'FROM SSW' in wd or 'FROM WSW' in wd:\n        return 'south west'\n    if 'FROM SE' in wd or 'FROM SSE' in wd or 'FROM ESE' in wd:\n        return 'south east'\n    if 'FROM NW' in wd or 'FROM NNW' in wd or 'FROM WNW' in wd:\n        return 'north west'\n    if 'FROM NE' in wd or 'FROM NNE' in wd or 'FROM ENE' in wd:\n        return 'north east'\n    \n    if 'NW' in wd or 'NORTHWEST' in wd:\n        return 'north west'\n    if 'NE' in wd or 'NORTH EAST' in wd:\n        return 'north east'\n    if 'SW' in wd or 'SOUTHWEST' in wd:\n        return 'south west'\n    if 'SE' in wd or 'SOUTHEAST' in wd:\n        return 'south east'\n\n    return 'none'\n\ndf_train['WindDirection'] = df_train['WindDirection'].apply(clean_wind_direction)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-24T03:35:05.902345Z","iopub.execute_input":"2021-11-24T03:35:05.902725Z","iopub.status.idle":"2021-11-24T03:35:06.506546Z","shell.execute_reply.started":"2021-11-24T03:35:05.902671Z","shell.execute_reply":"2021-11-24T03:35:06.505840Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"**Unhide above cell**","metadata":{}},{"cell_type":"markdown","source":"### Handling humidity and Temperature\nSince we are working with a time series, we have a nice benifit that there is a trick to filling empty vales\nYou can just use something called \"forward filling\", which just fills all empty values with the last known value\nSay at 3:00pm I know that the temp was 80 degrees, and then didn't record for 10 hours\nForward will will just fill everything for the next 10 hours with 80 degrees","metadata":{}},{"cell_type":"code","source":"df_train['Humidity'].fillna(method='ffill', inplace=True)\ndf_train['Temperature'].fillna(method='ffill', inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:35:06.507626Z","iopub.execute_input":"2021-11-24T03:35:06.508015Z","iopub.status.idle":"2021-11-24T03:35:06.519841Z","shell.execute_reply.started":"2021-11-24T03:35:06.507959Z","shell.execute_reply":"2021-11-24T03:35:06.518728Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Handling Game wheather and Stadium types\nFor some other values, we are going to build what is called a na map\nIts just a python dictionary that maps columns to the rule for filling t hem","metadata":{}},{"cell_type":"code","source":"na_map = {\n    # What is the average orientation of the playrees\n    'Orientation': df_train['Orientation'].mean(),\n    # Average direction \n    'Dir': df_train['Dir'].mean(),\n    # Average # of defenders in the box (# of defenders directly opposing person with the ball)\n    'DefendersInTheBox': np.math.ceil(df_train['DefendersInTheBox'].mean()),\n    # What formation the team is using is really important, but often some teams use custom formations\n    # In the case we will just say that we don't know\n    'OffenseFormation': 'UNKNOWN'\n}\n\ndf_train.fillna(na_map, inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:35:06.521182Z","iopub.execute_input":"2021-11-24T03:35:06.521460Z","iopub.status.idle":"2021-11-24T03:35:06.655046Z","shell.execute_reply.started":"2021-11-24T03:35:06.521410Z","shell.execute_reply":"2021-11-24T03:35:06.654092Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_train['GameWeather'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:35:06.656263Z","iopub.execute_input":"2021-11-24T03:35:06.656490Z","iopub.status.idle":"2021-11-24T03:35:06.723041Z","shell.execute_reply.started":"2021-11-24T03:35:06.656456Z","shell.execute_reply":"2021-11-24T03:35:06.722295Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Yeah this is insane, there are only 4 weather types that really matter\nrain, overcast, clear, snow, and if players are climate controlled (indoors etc)\nHere, we will make a function that first creates a list of all the different types we have \n(Unfortunately if you're doing this on your own you'll need to find each of these yourself)\nThen, just determine which box the weather belongs to and change it to that","metadata":{}},{"cell_type":"code","source":"def group_game_weather(weather):\n    rain = [\n        'Rainy', 'Rain Chance 40%', 'Showers',\n        'Cloudy with periods of rain, thunder possible. Winds shifting to WNW, 10-20 mph.',\n        'Scattered Showers', 'Cloudy, Rain', 'Rain shower', 'Light Rain', 'Rain'\n    ]\n    overcast = [\n        'Cloudy, light snow accumulating 1-3\"', 'Party Cloudy', 'Cloudy, chance of rain',\n        'Coudy', 'Cloudy, 50% change of rain', 'Rain likely, temps in low 40s.',\n        'Cloudy and cold', 'Cloudy, fog started developing in 2nd quarter',\n        'Partly Clouidy', '30% Chance of Rain', 'Mostly Coudy', 'Cloudy and Cool',\n        'cloudy', 'Partly cloudy', 'Overcast', 'Hazy', 'Mostly cloudy', 'Mostly Cloudy',\n        'Partly Cloudy', 'Cloudy'\n    ]\n    clear = [\n        'Partly clear', 'Sunny and clear', 'Sun & clouds', 'Clear and Sunny',\n        'Sunny and cold', 'Sunny Skies', 'Clear and Cool', 'Clear and sunny',\n        'Sunny, highs to upper 80s', 'Mostly Sunny Skies', 'Cold',\n        'Clear and warm', 'Sunny and warm', 'Clear and cold', 'Mostly sunny',\n        'T: 51; H: 55; W: NW 10 mph', 'Clear Skies', 'Clear skies', 'Partly sunny',\n        'Fair', 'Partly Sunny', 'Mostly Sunny', 'Clear', 'Sunny'\n    ]\n    snow  = ['Heavy lake effect snow', 'Snow']\n    none  = ['N/A Indoor', 'Indoors', 'Indoor', 'N/A (Indoors)', 'Controlled Climate']\n    \n    if weather in rain:\n        return 'rain'\n    elif weather in overcast:\n        return 'overcast'\n    elif weather in clear:\n        return 'clear'\n    elif weather in snow:\n        return 'snow'\n    elif weather in none:\n        return 'none'\n    \n    return 'none'\n\ndf_train['GameWeather'] = df_train['GameWeather'].apply(group_game_weather)\n\ndf_train['FieldPosition'] = np.where(df_train['YardLine'] == 50, df_train['PossessionTeam'], df_train['FieldPosition'])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T03:35:06.724363Z","iopub.execute_input":"2021-11-24T03:35:06.724632Z","iopub.status.idle":"2021-11-24T03:35:07.652924Z","shell.execute_reply.started":"2021-11-24T03:35:06.724578Z","shell.execute_reply":"2021-11-24T03:35:07.651999Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Stadium types\nAgain, let's reduce the number of stadiums types the same way we did with weather.\nThere is no difference between \"Indoors\" and \"Indoor\" obviously","metadata":{}},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:35:07.654078Z","iopub.execute_input":"2021-11-24T03:35:07.654334Z","iopub.status.idle":"2021-11-24T03:35:08.028405Z","shell.execute_reply.started":"2021-11-24T03:35:07.654292Z","shell.execute_reply":"2021-11-24T03:35:08.027420Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def group_stadium_types(stadium):\n    outdoor       = [\n        'Outdoor', 'Outdoors', 'Cloudy', 'Heinz Field', \n        'Outdor', 'Ourdoor', 'Outside', 'Outddors', \n        'Outdoor Retr Roof-Open', 'Oudoor', 'Bowl'\n    ]\n    indoor_closed = [\n        'Indoors', 'Indoor', 'Indoor, Roof Closed', 'Indoor, Roof Closed', \n        'Retractable Roof', 'Retr. Roof-Closed', 'Retr. Roof - Closed', 'Retr. Roof Closed',\n    ]\n    indoor_open   = ['Indoor, Open Roof', 'Open', 'Retr. Roof-Open', 'Retr. Roof - Open']\n    dome_closed   = ['Dome', 'Domed, closed', 'Closed Dome', 'Domed', 'Dome, closed']\n    dome_open     = ['Domed, Open', 'Domed, open']\n    \n    if stadium in outdoor:\n        return 'outdoor'\n    elif stadium in indoor_closed:\n        return 'indoor closed'\n    elif stadium in indoor_open:\n        return 'indoor open'\n    elif stadium in dome_closed:\n        return 'dome closed'\n    elif stadium in dome_open:\n        return 'dome open'\n    else:\n        return 'unknown'\n    \ndf_train['StadiumType'] = df_train['StadiumType'].apply(group_stadium_types)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-24T03:35:08.029796Z","iopub.execute_input":"2021-11-24T03:35:08.030080Z","iopub.status.idle":"2021-11-24T03:35:08.484619Z","shell.execute_reply.started":"2021-11-24T03:35:08.030029Z","shell.execute_reply":"2021-11-24T03:35:08.483766Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"**Unhide above cell**\n- The stadium types and weather is cleaned.","metadata":{}},{"cell_type":"markdown","source":"### BirthDate, GameHour and Time\n- we will extract and consider Birth year of each player\n- We will extract and consider Hour from GameClock\n- We will calucate TimeDelta by subtracting TimeSnap from TimeHandoff .\n","metadata":{}},{"cell_type":"code","source":"df_train['TimeHandoff'] = df_train['TimeHandoff'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\ndf_train['TimeSnap'] = df_train['TimeSnap'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\ndf_train['TimeDelta'] = df_train.apply(lambda row: (row['TimeHandoff'] - row['TimeSnap']).total_seconds(), axis=1)\ndf_train.drop(['TimeSnap','TimeHandoff'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:35:08.485953Z","iopub.execute_input":"2021-11-24T03:35:08.486252Z","iopub.status.idle":"2021-11-24T03:36:14.947732Z","shell.execute_reply.started":"2021-11-24T03:35:08.486194Z","shell.execute_reply":"2021-11-24T03:36:14.946486Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df_train['BirthYear']=df_train['PlayerBirthDate'].apply(lambda x : int(x.split('/')[2]))\ndf_train['GameHour']=df_train['GameClock'].apply(lambda x : int(x.split(':')[0]))\n\ndf_train.drop(['PlayerBirthDate',\"GameClock\"],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:14.949305Z","iopub.execute_input":"2021-11-24T03:36:14.949574Z","iopub.status.idle":"2021-11-24T03:36:16.243140Z","shell.execute_reply.started":"2021-11-24T03:36:14.949530Z","shell.execute_reply":"2021-11-24T03:36:16.242303Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"df_train['PlayerHeight']=df_train['PlayerHeight'].apply(lambda x : np.mean(list(map(int,x.split('-')))))\n#df_train.drop('PlayerHeight',axis=1,inplace=True)                                                       ","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:16.244347Z","iopub.execute_input":"2021-11-24T03:36:16.244573Z","iopub.status.idle":"2021-11-24T03:36:24.831337Z","shell.execute_reply.started":"2021-11-24T03:36:16.244533Z","shell.execute_reply":"2021-11-24T03:36:24.830348Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### DefensePersonnel\n","metadata":{}},{"cell_type":"markdown","source":" - We will split the DefensePersonnel variable into **DL , LB ,  BL , OL**\n - We will filter them and store them as int variables.","metadata":{}},{"cell_type":"code","source":"def process_defense(x):\n    num=[]\n    num=x.split(',')\n    dl=int(num[0].split(' ')[0])\n    lb=int(num[1].split(' ')[1])\n    db=int(num[2].split(' ')[1])\n    if(len(num)>3):\n         ol=int(num[3].split(' ')[1])\n    else:\n         ol=0\n    return [dl,lb,db,ol]\n\nvalues=df_train['DefensePersonnel'].apply(process_defense)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:24.832501Z","iopub.execute_input":"2021-11-24T03:36:24.832737Z","iopub.status.idle":"2021-11-24T03:36:26.371159Z","shell.execute_reply.started":"2021-11-24T03:36:24.832696Z","shell.execute_reply":"2021-11-24T03:36:26.370295Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"u,v,x,y=list(map(list,zip(*values)))","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:26.372462Z","iopub.execute_input":"2021-11-24T03:36:26.372707Z","iopub.status.idle":"2021-11-24T03:36:27.115411Z","shell.execute_reply.started":"2021-11-24T03:36:26.372665Z","shell.execute_reply":"2021-11-24T03:36:27.114495Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"df_train['DL']=u\ndf_train['LB']=v\ndf_train['BL']=x\ndf_train['OL']=y\ndf_train.drop(['DefensePersonnel'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:27.116553Z","iopub.execute_input":"2021-11-24T03:36:27.116783Z","iopub.status.idle":"2021-11-24T03:36:28.058814Z","shell.execute_reply.started":"2021-11-24T03:36:27.116740Z","shell.execute_reply":"2021-11-24T03:36:28.057853Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:28.060001Z","iopub.execute_input":"2021-11-24T03:36:28.060244Z","iopub.status.idle":"2021-11-24T03:36:28.066341Z","shell.execute_reply.started":"2021-11-24T03:36:28.060206Z","shell.execute_reply":"2021-11-24T03:36:28.065030Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Encoding values..\n- In this step we will encode all categorical variables.\n- we will use Label Encoding for this.\n- we will store each of instances in a dictionary for later use during test set preparation.","metadata":{}},{"cell_type":"code","source":"new_obj=[]\nfor c in df_train.columns:\n    if(df_train[c].dtype != int):\n            try:\n                df_train[c]=df_train[c].astype('float16')\n            except:\n                new_obj.append(c)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:28.067930Z","iopub.execute_input":"2021-11-24T03:36:28.068279Z","iopub.status.idle":"2021-11-24T03:36:28.272331Z","shell.execute_reply.started":"2021-11-24T03:36:28.068205Z","shell.execute_reply":"2021-11-24T03:36:28.271519Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"lbdic={}\nfor c in new_obj:\n    lb=LabelEncoder()\n    lb=lb.fit(df_train[c].values)\n    lbdic[c]=lb\n    df_train[c]=lb.transform(df_train[c].values)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:28.274742Z","iopub.execute_input":"2021-11-24T03:36:28.274947Z","iopub.status.idle":"2021-11-24T03:36:32.765921Z","shell.execute_reply.started":"2021-11-24T03:36:28.274915Z","shell.execute_reply":"2021-11-24T03:36:32.764804Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## Data preparation","metadata":{}},{"cell_type":"markdown","source":"In the below session we will process and prepare data inorder to feed it to our model.\n- First,we will drop some columns which are not relevant.\n- we will classify our varibles into three lists ,is one,two and more.\n- one contains variable having unique values.\n- more contains variables having more than 2 different values.","metadata":{}},{"cell_type":"code","source":"columns_drop=['GameId','PlayId','NflId','NflIdRusher']\none=[]\ntwo=[]\nmore=[]\nfor col in df_train.drop(columns_drop,axis=1).columns:\n    if df_train[col][:22].nunique() <2:\n        one.append(col)\n    elif df_train[col][:22].nunique() <=2:\n        two.append(col)\n    else:\n        more.append(col)\n        ","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:32.767943Z","iopub.execute_input":"2021-11-24T03:36:32.768374Z","iopub.status.idle":"2021-11-24T03:36:33.136541Z","shell.execute_reply.started":"2021-11-24T03:36:32.768261Z","shell.execute_reply":"2021-11-24T03:36:33.135778Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"print('The number of attributes for preprocessing =',len(one)+len(two)+len(more))","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:33.137841Z","iopub.execute_input":"2021-11-24T03:36:33.138314Z","iopub.status.idle":"2021-11-24T03:36:33.143841Z","shell.execute_reply.started":"2021-11-24T03:36:33.138272Z","shell.execute_reply":"2021-11-24T03:36:33.142780Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"During the next steps we will actually convert the data to our required form.Each row in the output dataframe will represent a game.Our target variable is Yards,which is the the yardage gained on the play.","metadata":{}},{"cell_type":"code","source":"# We're going to start by appending the variables that have more than two unique values\n# Remember, every \"example\" is actually one of 11 timesteps from two seperate games, \n# For a total of 22 datapoints per player\nnew_cols=[]\nfor col in more:\n    for i in range(0,11):\n        new_cols.append(str(col)+'A'+str(i))\n    for i in range(0,11):\n         new_cols.append(str(col)+'B'+str(i))\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:33.145449Z","iopub.execute_input":"2021-11-24T03:36:33.145791Z","iopub.status.idle":"2021-11-24T03:36:33.153406Z","shell.execute_reply.started":"2021-11-24T03:36:33.145723Z","shell.execute_reply":"2021-11-24T03:36:33.152493Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"train=pd.DataFrame()\nx=np.tile(np.arange(0,22),14)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:33.154882Z","iopub.execute_input":"2021-11-24T03:36:33.155389Z","iopub.status.idle":"2021-11-24T03:36:33.166598Z","shell.execute_reply.started":"2021-11-24T03:36:33.155332Z","shell.execute_reply":"2021-11-24T03:36:33.165643Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Now we build the targets\nout=[]\nfor c in more:\n    for  i in range(0,22):\n         out.append(df_train[i:len(df_train):22][c].values)\n               \nfor col in zip(new_cols,np.arange(len(out))):\n    train[col]=out[i]\nout=np.array(out).transpose()","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:33.172211Z","iopub.execute_input":"2021-11-24T03:36:33.172909Z","iopub.status.idle":"2021-11-24T03:36:33.888872Z","shell.execute_reply.started":"2021-11-24T03:36:33.172846Z","shell.execute_reply":"2021-11-24T03:36:33.887964Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"train=pd.DataFrame(data=out,columns=new_cols)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:33.894831Z","iopub.execute_input":"2021-11-24T03:36:33.895145Z","iopub.status.idle":"2021-11-24T03:36:33.901277Z","shell.execute_reply.started":"2021-11-24T03:36:33.895092Z","shell.execute_reply":"2021-11-24T03:36:33.900329Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:33.903102Z","iopub.execute_input":"2021-11-24T03:36:33.903480Z","iopub.status.idle":"2021-11-24T03:36:33.935315Z","shell.execute_reply.started":"2021-11-24T03:36:33.903415Z","shell.execute_reply":"2021-11-24T03:36:33.934471Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### In the next step we will group our dataframe using PlayId and select values from columns which are labelled as **one**.","metadata":{}},{"cell_type":"code","source":"df_one=df_train.groupby(['PlayId'])[one].first()\nfor col in df_one.columns:\n    train[col]=df_one[col].values","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:33.936724Z","iopub.execute_input":"2021-11-24T03:36:33.937250Z","iopub.status.idle":"2021-11-24T03:36:34.187730Z","shell.execute_reply.started":"2021-11-24T03:36:33.937026Z","shell.execute_reply":"2021-11-24T03:36:34.186954Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"Lets check our target variable distribution..","metadata":{}},{"cell_type":"code","source":"sns.distplot(train['Yards'].values)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:34.189422Z","iopub.execute_input":"2021-11-24T03:36:34.189948Z","iopub.status.idle":"2021-11-24T03:36:34.470289Z","shell.execute_reply.started":"2021-11-24T03:36:34.189738Z","shell.execute_reply":"2021-11-24T03:36:34.469277Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"not_object=[]\nobj=[]\nfor col in more+one:\n    if df_train[col].dtype != 'object':\n        not_object.append(col)\n    else:\n        obj.append(col)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:34.471663Z","iopub.execute_input":"2021-11-24T03:36:34.471915Z","iopub.status.idle":"2021-11-24T03:36:34.477070Z","shell.execute_reply.started":"2021-11-24T03:36:34.471872Z","shell.execute_reply":"2021-11-24T03:36:34.475935Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"- We will split our train and taget variables.","metadata":{}},{"cell_type":"code","source":"# Split the features and target variables\nX=train.drop('Yards',axis=1)\ny=train['Yards']\n","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:34.478320Z","iopub.execute_input":"2021-11-24T03:36:34.478548Z","iopub.status.idle":"2021-11-24T03:36:34.512716Z","shell.execute_reply.started":"2021-11-24T03:36:34.478506Z","shell.execute_reply":"2021-11-24T03:36:34.511744Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## Keras model\n- In this section we will use keras Sequencial models to build our model.\n- We will use Dropout and BatchNormalization.\n","metadata":{}},{"cell_type":"code","source":"def create_model():\n    model=Sequential()\n    model.add(Dense(356,input_shape=[X.shape[1]],activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(.4))\n    model.add(Dense(200,activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(.4))\n    model.add(Dense(256,activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(.3))\n    model.add(Dense(212,activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(.3))\n    model.add(Dense(199,activation='sigmoid'))\n\n    optimizer=Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999)\n    model.compile(optimizer=optimizer,loss=['mse'],metrics=['accuracy'])\n    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n                                                patience=3, \n                                                verbose=1, \n                                                factor=0.5, \n                                                min_lr=0.00001)\n    return model\nmodel = create_model()","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:34.514132Z","iopub.execute_input":"2021-11-24T03:36:34.514443Z","iopub.status.idle":"2021-11-24T03:36:35.039526Z","shell.execute_reply.started":"2021-11-24T03:36:34.514389Z","shell.execute_reply":"2021-11-24T03:36:35.038355Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"- We will also convert our Target variable to appropriate form.Ie 199 x 1 array.","metadata":{}},{"cell_type":"code","source":"def transform_y(X_train,y_train):\n    Y_train=np.zeros((X_train.shape[0],199))\n    for i,yard in enumerate(y_train):\n        Y_train[i, yard+99:] = np.ones(shape=(1, 100-yard))\n    \n    return Y_train\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:36:35.041286Z","iopub.execute_input":"2021-11-24T03:36:35.041615Z","iopub.status.idle":"2021-11-24T03:36:35.048567Z","shell.execute_reply.started":"2021-11-24T03:36:35.041563Z","shell.execute_reply":"2021-11-24T03:36:35.047388Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## KFold cross validation..\nEveryone remember how this works?","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nkfold=KFold(n_splits=3,shuffle=True)\n\nfor train_ind,val in kfold.split(X,y):\n    \n    x_train,xval = X.iloc[train_ind],X.iloc[val]\n    y_train,yval= y.iloc[train_ind],y.iloc[val]\n    \n    y_train=transform_y(x_train,y_train)\n    y_val=transform_y(xval,yval)\n    \n    model=None\n    model=create_model()\n    \n    history=model.fit(x_train,y_train,epochs=20,validation_data=[xval,y_val],verbose=1)\n    print('validation accuracy : {}'.format(np.mean(history.history['val_accuracy'])))","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-24T03:36:35.050326Z","iopub.execute_input":"2021-11-24T03:36:35.050743Z","iopub.status.idle":"2021-11-24T03:42:15.131905Z","shell.execute_reply.started":"2021-11-24T03:36:35.050669Z","shell.execute_reply":"2021-11-24T03:42:15.131176Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"- We will also use learning rate reducer to reduce learning rate and help it converge easily to minima point.","metadata":{}},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"code","source":"fig,ax=plt.subplots(2,1)\nfig.set_size_inches((5,5))\nepochs=20\nx=range(1,1+epochs)\nax[0].plot(x,history.history['loss'],color='red')\nax[0].plot(x,history.history['val_loss'],color='blue')\n\nax[1].plot(x,history.history['accuracy'],color='red')\nax[1].plot(x,history.history['val_accuracy'],color='blue')\nax[0].legend(['trainng loss','validation loss'])\nax[1].legend(['trainng acc','validation acc'])\nplt.xlabel('Number of epochs')\nplt.ylabel('accuracy')","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:42:15.133235Z","iopub.execute_input":"2021-11-24T03:42:15.133523Z","iopub.status.idle":"2021-11-24T03:42:15.477638Z","shell.execute_reply.started":"2021-11-24T03:42:15.133470Z","shell.execute_reply":"2021-11-24T03:42:15.476832Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"We have lot to improve !","metadata":{}},{"cell_type":"markdown","source":"## Making Prediction..","metadata":{}},{"cell_type":"markdown","source":"- This function is to prepare and predict on test data.\n- We will have to replicate all the data processing steps that we have done above in this function.\n- This ensures that our data is made fit for prediction.\n- In this competition we will have to use **env.iter_test()** and **env.predict()**.\n","metadata":{}},{"cell_type":"code","source":"def make_prediction(test,sample,env,model,df_train):\n    \n    na_map = {\n    'Orientation': df_train['Orientation'].mean(),\n    'Dir': df_train['Dir'].mean(),\n    'DefendersInTheBox': 7.0,\n    'OffenseFormation': 'UNKNOWN','WindSpeed':df_train['WindSpeed'].mean()\n    }\n\n    test.fillna(na_map, inplace=True)\n    test['Temperature'].fillna(61.0,inplace=True)\n    test['WindSpeed']=test['WindSpeed'].apply(windspeed)\n    #test['WindSpeed'].fillna(df_train['WindSpeed'].mean(),inplace=True)\n\n    test['GameWeather'] = test['GameWeather'].apply(group_game_weather)\n    test['FieldPosition'] = np.where(test['YardLine'] == 50, test['PossessionTeam'], test['FieldPosition'])\n    test['StadiumType'] = test['StadiumType'].apply(group_stadium_types)\n    test['WindDirection'] = test['WindDirection'].apply(clean_wind_direction)\n    \n    test['TimeHandoff'] = test['TimeHandoff'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n    test['TimeSnap'] = test['TimeSnap'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n    test['TimeDelta'] = test.apply(lambda row: (row['TimeHandoff'] - row['TimeSnap']).total_seconds(), axis=1)\n    test.drop(['TimeSnap','TimeHandoff'],axis=1,inplace=True)\n   \n    test['PlayerHeight']=test['PlayerHeight'].apply(lambda x : np.mean(list(map(int,x.split('-')))))\n\n\n\n\n    test['BirthYear']=test['PlayerBirthDate'].apply(lambda x : int(x.split('/')[2]))\n    test['GameHour']=test['GameClock'].apply(lambda x : int(x.split(':')[0]))\n    test.drop(['PlayerBirthDate',\"GameClock\"],axis=1,inplace=True)\n\n    values=test['DefensePersonnel'].apply(process_defense)\n    u,v,x,y=list(map(list,zip(*values)))\n    test['DL']=u\n    test['LB']=v\n    test['BL']=x\n    test['OL']=y\n    test.drop(['DefensePersonnel'],axis=1,inplace=True)\n    \n    new_obj=[]\n    for c in test.columns:\n        if(test[c].dtype != int):\n                try:\n                    test[c]=test[c].astype('float16')\n                except:\n                    new_obj.append(c)\n\n    for c in new_obj:\n        try:\n            test[c]=lbdic[c].transform(test[c].values)\n        except:\n            l=LabelEncoder()\n            test[c]=l.fit_transform(test[c].values)\n            \n    \n    columns_drop=['GameId','PlayId','NflId','NflIdRusher']\n    one=[]\n    two=[]\n    more=[]\n    for col in test.drop(columns_drop,axis=1).columns:\n        if test[col][:22].nunique() <2:\n            one.append(col)\n        elif test[col][:22].nunique() <=2:\n            two.append(col)\n        else:\n            more.append(col)\n        \n\n    new_cols=[]\n    for col in more:\n        for i in range(0,11):\n            new_cols.append(str(col)+'A'+str(i))\n        for i in range(0,11):\n             new_cols.append(str(col)+'B'+str(i))\n\n    \n\n    out=[]\n    for c in more:\n        out.append(test[c].values)\n    \n   \n    new_out=[]\n    for i in out:\n        for j in i:\n            new_out.append(j)\n\n    new_test=pd.DataFrame(data=[new_out],columns=new_cols)\n\n    df_one=test.groupby(['PlayId'])[one].first()\n    for col in df_one.columns:\n        new_test[col]=df_one[col].values\n\n    \n    new_test.fillna(na_map,inplace=True)\n    new_test['Temperature'].fillna(61.0,inplace=True)\n        \n    y_pred=np.zeros((1,199))\n    \n    y_pred = model.predict(new_test)\n    \n        \n    for pred in y_pred:\n        prev = 0\n        for i in range(len(pred)):\n            if pred[i]<prev:\n                pred[i]=prev\n            prev=pred[i]\n    \n    y_pred[:, -1] = np.ones(shape=(y_pred.shape[0], 1))\n    y_pred[:, 0] = np.zeros(shape=(y_pred.shape[0], 1))\n  \n    pred=pd.DataFrame(data=y_pred,columns=sample.columns)\n    env.predict(pred)\n\n    return y_pred\n\n        \n","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:42:15.478927Z","iopub.execute_input":"2021-11-24T03:42:15.479153Z","iopub.status.idle":"2021-11-24T03:42:15.505176Z","shell.execute_reply.started":"2021-11-24T03:42:15.479113Z","shell.execute_reply":"2021-11-24T03:42:15.504193Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"### Submitting to competition..","metadata":{}},{"cell_type":"code","source":"for test, sample in tqdm.tqdm(env.iter_test()):\n    make_prediction(test,sample,env,model,df_train)\n    \nenv.write_submission_file()","metadata":{"execution":{"iopub.status.busy":"2021-11-24T03:42:15.506463Z","iopub.execute_input":"2021-11-24T03:42:15.506783Z","iopub.status.idle":"2021-11-24T03:45:00.563097Z","shell.execute_reply.started":"2021-11-24T03:42:15.506677Z","shell.execute_reply":"2021-11-24T03:45:00.561771Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}